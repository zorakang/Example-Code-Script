{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNE-6 model factor data proccessing\n",
    "def factor_data_process(melt_table, factor):\n",
    "    #depolarization\n",
    "    melt_table[factor] = melt_table.groupby('pubDate')[factor].transform(winsorize,\n",
    "                                            qrange=[0.025, 0.975], inclusive=True, inf2nan=True, axis=0)\n",
    "    print('winsorize finished')\n",
    "    #Completion of missing values\n",
    "    missing_value_rows = melt_table[melt_table[factor].isnull()]\n",
    "    if not missing_value_rows.empty:\n",
    "        missing_value_rows_index = missing_value_rows.index.tolist()\n",
    "        missing_stock_code = missing_value_rows['code'].tolist()\n",
    "        missing_date = missing_value_rows['pubDate'].tolist()\n",
    "        fill_values = []\n",
    "        for i in np.arange(len(missing_stock_code)):\n",
    "            industry_dict = get_industry(missing_stock_code[i], date = missing_date[i])\n",
    "            if len(list(industry_dict[missing_stock_code[i]].keys())) != 0:\n",
    "                if 'sw_l1' in industry_dict[missing_stock_code[i]]:\n",
    "                    industry = industry_dict[missing_stock_code[i]]['sw_l1']['industry_code']\n",
    "                else :\n",
    "                    first_key = list(industry_dict[missing_stock_code[i]].keys())[0]\n",
    "                    industry = industry_dict[missing_stock_code[i]][first_key]['industry_code']\n",
    "                industry_stocks = get_industry_stocks(industry, date = missing_date[i])\n",
    "                industry_dic = lib.get_data(industry_stocks, [missing_date[i]], [factor], \n",
    "                                                    query_limit=False)\n",
    "                melt_df = pd.melt(industry_dic[factor].reset_index(),id_vars=['index'],\n",
    "                                      value_vars=securities,var_name='code',value_name=factor)\n",
    "                val = np.mean(melt_df[factor])\n",
    "                melt_table.at[missing_value_rows_index[i], factor] = val\n",
    "    melt_table = melt_table.dropna() #missing values in related industries   \n",
    "    print('nan value fill finished')\n",
    "    #neutralization\n",
    "    for pubDate, group in melt_table.groupby('pubDate'):\n",
    "        group.set_index('code', inplace=True)\n",
    "        group[factor] = neutralize(group[factor], how=['market_cap'], date=pubDate, axis=0)\n",
    "        group.reset_index(inplace=True)\n",
    "        group = group.rename(columns = {'index':'code'})\n",
    "    print('neutralize finished')     \n",
    "    #standardization\n",
    "    melt_table[factor] = standardlize(melt_table[factor], inf2nan=True, axis=0)\n",
    "    print('standardlize finished')\n",
    "    return melt_table\n",
    "    \n",
    "#earnings yield\n",
    "def rates_attain(securities, start_date, end_date):\n",
    "    daily_price = get_price(securities,start_date = start_date, end_date = end_date,fields=('close',\n",
    "                                                                                        ))['close']\n",
    "    rates = daily_price/daily_price.shift(1) - 1\n",
    "    rates_tbl = pd.melt(rates.reset_index(),id_vars=['index'],value_vars=securities, \n",
    "                   var_name='code', value_name='rates').rename(columns = {'index':'pubDate'})\n",
    "    rates_tbl['pubDate'] = pd.to_datetime(rates_tbl['pubDate'])\n",
    "    #Remove unlisted/delisted\n",
    "    rates_tbl = rates_tbl[rates_tbl['rates'] != 0.0]\n",
    "    return rates_tbl.dropna()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def data_cleaning(new_train):\n",
    "    new_train['subject'] = new_train['subject'].astype(str)\n",
    "    new_train['email'] = new_train['email'].astype(str)\n",
    "    #subject\n",
    "    new_train['cap_subject'] = new_train['subject'].map(lambda calc:len(re.sub(r'[^A-Z]+',\"\",calc)))\n",
    "    new_train['mark_subject'] = new_train['subject'].map(lambda calc:len(re.sub(r'[^!]+',\"\",calc)))\n",
    "    new_train['reply'] = new_train['subject'].map(lambda calc:len(re.findall(r'Re:', calc)))\n",
    "    new_train['Free'] = new_train['subject'].map(lambda calc:len(re.findall(r'FREE', calc)))\n",
    "    new_train['star'] = new_train['subject'].map(lambda calc:len(re.sub(r'[^*]+',\"\",calc)))\n",
    "    new_train['turn_subject'] = new_train['subject'].map(lambda calc:len(re.sub(r'[^\\n]+',\"\",calc)))\n",
    "    new_train[':_subject'] = new_train['subject'].map(lambda calc:len(re.sub(r'[^:]+',\"\",calc)))\n",
    "    new_train['[]'] = new_train['subject'].map(lambda calc:len(re.sub(r'[^\\[\\]]+',\"\",calc)))\n",
    "    #body\n",
    "    new_train['cap_body'] = new_train['email'].map(lambda calc:len(re.sub(r'[^A-Z]+',\"\",calc))/len(calc))\n",
    "    new_train['turn_body'] = new_train['email'].map(lambda calc:len(re.sub(r'[^\\n]+',\"\",calc)))\n",
    "    new_train['dollar_sign'] = new_train['email'].map(lambda calc:len(re.findall(r'$', calc)))\n",
    "    new_train['hash'] = new_train['email'].map(lambda calc:len(re.findall(r\"\\\\\", calc)))\n",
    "    new_train['#'] = new_train['email'].map(lambda calc:len(re.findall(r'#', calc)))\n",
    "    new_train['-'] = new_train['email'].map(lambda calc:len(re.findall(r\"-\", calc)))\n",
    "    new_train['?'] = new_train['email'].map(lambda calc:len(re.findall(r'\\?', calc)))\n",
    "    new_train['/'] = new_train['email'].map(lambda calc:len(re.findall(r'\\/', calc)))\n",
    "    new_train[':'] = new_train['email'].map(lambda calc:len(re.findall(r'\\:', calc)))\n",
    "    some_words = ['drug', 'bank', 'prescription', 'memo', 'private', '<html>','dear','100']\n",
    "    X_train = words_in_texts(some_words, new_train['email'])\n",
    "    X_train = np.array([np.append(X_train[i],new_train['mark_subject'][i]) for i in range(len(X_train))])\n",
    "    X_train = np.array([np.append(X_train[i],new_train['cap_body'][i]) for i in range(len(X_train))])\n",
    "    X_train = np.array([np.append(X_train[i],new_train['cap_subject'][i]) for i in range(len(X_train))])\n",
    "    X_train = np.array([np.append(X_train[i],new_train['turn_body'][i]) for i in range(len(X_train))])\n",
    "    X_train = np.array([np.append(X_train[i],new_train['reply'][i]) for i in range(len(X_train))])\n",
    "    X_train = np.array([np.append(X_train[i],new_train['dollar_sign'][i]) for i in range(len(X_train))])\n",
    "    X_train = np.array([np.append(X_train[i],new_train['hash'][i]) for i in range(len(X_train))])\n",
    "    X_train = np.array([np.append(X_train[i],new_train['star'][i]) for i in range(len(X_train))])\n",
    "    X_train = np.array([np.append(X_train[i],new_train['#'][i]) for i in range(len(X_train))])\n",
    "    X_train = np.array([np.append(X_train[i],new_train['-'][i]) for i in range(len(X_train))])\n",
    "    X_train = np.array([np.append(X_train[i],new_train['?'][i]) for i in range(len(X_train))])\n",
    "    X_train = np.array([np.append(X_train[i],new_train['[]'][i]) for i in range(len(X_train))])\n",
    "    return new_train, X_train\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "new_train, X_train = data_cleaning(train)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "training_accuracy = model.score(X_train, Y_train)\n",
    "print(\"Training Accuracy: \", training_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(train_data, train_label, val_data, val_label, train_sizes):\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    #flatten_data:\n",
    "    train_data = train_data.reshape((train_data.shape[0], -1))\n",
    "    val_data = val_data.reshape((val_data.shape[0], -1))\n",
    "\n",
    "    for size in train_sizes:\n",
    "        train_data_subset = train_data[:size]\n",
    "        train_label_subset = train_label[:size]\n",
    "\n",
    "        model = SVC(kernel='linear')\n",
    "        model.fit(train_data_subset, train_label_subset)\n",
    "\n",
    "        # Predict on the training subset and validation set\n",
    "        train_label_pred = model.predict(train_data_subset)\n",
    "        val_label_pred = model.predict(val_data)\n",
    "\n",
    "        # Calculate the accuracies\n",
    "        train_accuracy = evaluation_metric(train_label_subset, train_label_pred)\n",
    "        val_accuracy = evaluation_metric(val_label, val_label_pred)\n",
    "\n",
    "        # Append to the results list\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        print(f\"Trained with {size} samples, Training accuracy: {train_accuracy}, Validation accuracy: {val_accuracy}\")\n",
    "\n",
    "    return train_accuracies, val_accuracies\n",
    "\n",
    "def plot_accuracies(train_sizes, train_accuracies, val_accuracies, title):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_sizes, train_accuracies, label='Training Accuracy')\n",
    "    plt.plot(train_sizes, val_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel('Number of Training Examples')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
